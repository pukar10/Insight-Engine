# backend/answer.py

"""
answer.py

Use Ollama + a local model (phi4) to answer questions
using chunks retrieved from the Chroma vector database.

High-level flow:
  1. Use search() to get the most relevant chunks for a question.
  2. Build a prompt that includes those chunks as context.
  3. Send that prompt to Ollama (phi4 model).
  4. Return the model's answer + the chunks used.
"""

from textwrap import dedent
from typing import List, Dict, Tuple

import ollama
from ollama import ResponseError

from backend.search import search


def build_prompt(question: str, chunks: List[Dict]) -> str:
    """
    Create a prompt string for the LLM, using the retrieved chunks as context.

    - question: the user's question
    - chunks:   list of dicts coming from search(), each with 'text', 'source', etc.
    """
    # Collect just the text of each chunk
    context_parts = [c["text"] for c in chunks]

    # Join them with separators so the model can see all of them
    context = "\n\n---\n\n".join(context_parts)

    prompt = dedent(
        f"""
        You are an assistant that answers questions using ONLY the context below.
        If the context does not contain the answer, say:
        "I don't know based on these documents."

        Context:
        {context}

        Question: {question}

        Answer clearly and concisely.
        """
    ).strip()

    return prompt


def answer_question(
    question: str,
    n_context_chunks: int = 5,
) -> Tuple[str, List[Dict]]:
    """
    Main function called by the Streamlit app.

    Steps:
      1. Retrieve top N chunks related to the question.
      2. Build a prompt with those chunks.
      3. Ask the phi4 model in Ollama for an answer.

    Returns:
      - answer_text: the text generated by the model
      - chunks:      the list of chunks that were used as context
    """

    # 1. Retrieve chunks from your Chroma index
    chunks = search(question, n_results=n_context_chunks)

    # Safety: if no chunks found, bail early
    if not chunks:
        return "I couldn't find any relevant context in your documents.", []
    
    # 2. Build the prompt using chunks
    prompt = build_prompt(question, chunks)

    # 3. Talk to Ollama.
    # IMPORTANT:
    #   We hard-code model="phi4" because you already tested this in the REPL
    #   and know it works. This avoids any mismatch in model names.
    try:
        response = ollama.chat(
            model="phi4",  # <-- this is the key line; same as your working REPL test
            messages=[{"role": "user", "content": prompt}],
        )
    except ResponseError as e:
        # This will show the *real* error text from Ollama in your terminal,
        # which is way more useful than just a stack trace.
        print("Ollama ResponseError:", e)
        # Re-raise so Streamlit still shows an error in the UI
        raise
    except Exception as e:
        # Catch any other weirdness
        print("Unexpected error talking to Ollama:", repr(e))
        raise

    # Ollama returns a dict; the actual text is in response["message"]["content"]
    answer_text = response["message"]["content"]

    return answer_text, chunks


# Optional: test this file directly with: python -m backend.answer
if __name__ == "__main__":
    test_question = "Summarize what my documents say about anything."
    text, used_chunks = answer_question(test_question, n_context_chunks=3)
    print("Answer:\n", text)
    print("\nChunks used:", len(used_chunks))
